"""
Processador de Base de Conhecimento
Processa documentos TXT, gera embeddings e armazena no Supabase
Similar ao processamento de reuni√µes mas otimizado para documentos est√°ticos
"""

import os
import hashlib
import json
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import re
from pathlib import Path

import numpy as np
from openai import OpenAI
from supabase import create_client, Client
from dotenv import load_dotenv

# Carrega vari√°veis de ambiente
load_dotenv()

class ProcessadorBaseConhecimento:
    """Processa documentos da base de conhecimento para busca sem√¢ntica"""
    
    def __init__(self):
        """Inicializa o processador com clientes OpenAI e Supabase"""
        self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.supabase_client: Client = create_client(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_ANON_KEY')
        )
        
        # Configura√ß√µes de chunking
        self.chunk_size = 1500  # Caracteres por chunk
        self.chunk_overlap = 200  # Overlap entre chunks
        self.embedding_model = "text-embedding-ada-002"
        
        print("‚úÖ Processador de Base de Conhecimento inicializado")
    
    def calcular_hash_documento(self, conteudo: str) -> str:
        """Calcula hash SHA256 do documento para controle de vers√£o"""
        return hashlib.sha256(conteudo.encode()).hexdigest()
    
    def limpar_texto(self, texto: str) -> str:
        """Limpa e normaliza o texto do documento"""
        # Remove m√∫ltiplos espa√ßos e quebras de linha
        texto = re.sub(r'\s+', ' ', texto)
        # Remove caracteres especiais mantendo pontua√ß√£o b√°sica
        texto = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)\[\]\"\'√°√©√≠√≥√∫√¢√™√¥√£√µ√ß√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á]', '', texto)
        return texto.strip()
    
    def criar_chunks(self, texto: str) -> List[Dict[str, any]]:
        """
        Divide o texto em chunks com overlap
        Retorna lista de dicion√°rios com conte√∫do e metadados
        """
        chunks = []
        texto_limpo = self.limpar_texto(texto)
        
        # Divide por senten√ßas para n√£o cortar no meio
        sentencas = re.split(r'(?<=[.!?])\s+', texto_limpo)
        
        chunk_atual = ""
        chunk_index = 0
        
        for sentenca in sentencas:
            # Se adicionar a senten√ßa exceder o tamanho, cria novo chunk
            if len(chunk_atual) + len(sentenca) > self.chunk_size and chunk_atual:
                chunks.append({
                    'conteudo': chunk_atual.strip(),
                    'chunk_index': chunk_index,
                    'chunk_size': len(chunk_atual.strip())
                })
                
                # Overlap: mant√©m parte do chunk anterior
                palavras_overlap = chunk_atual.split()[-30:]  # ~200 caracteres
                chunk_atual = ' '.join(palavras_overlap) + ' ' + sentenca
                chunk_index += 1
            else:
                chunk_atual += ' ' + sentenca
        
        # Adiciona √∫ltimo chunk se houver
        if chunk_atual.strip():
            chunks.append({
                'conteudo': chunk_atual.strip(),
                'chunk_index': chunk_index,
                'chunk_size': len(chunk_atual.strip())
            })
        
        print(f"üìÑ Documento dividido em {len(chunks)} chunks")
        return chunks
    
    def gerar_embedding(self, texto: str) -> List[float]:
        """Gera embedding usando OpenAI"""
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=texto
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"‚ùå Erro ao gerar embedding: {e}")
            raise
    
    def detectar_tipo_documento(self, conteudo: str, nome_arquivo: str) -> Tuple[str, str]:
        """
        Detecta o tipo e categoria do documento baseado no conte√∫do e nome
        Retorna (tipo_documento, categoria)
        """
        conteudo_lower = conteudo.lower()
        nome_lower = nome_arquivo.lower()
        
        # Detec√ß√£o por palavras-chave
        if any(palavra in conteudo_lower for palavra in ['manual', 'instru√ß√µes', 'procedimento operacional']):
            tipo = 'manual'
            if 't√©cnico' in conteudo_lower:
                categoria = 'manual_tecnico'
            elif 'usu√°rio' in conteudo_lower:
                categoria = 'manual_usuario'
            else:
                categoria = 'manual_geral'
        
        elif any(palavra in conteudo_lower for palavra in ['estatuto', 'regimento', 'regulamento']):
            tipo = 'estatuto'
            categoria = 'estatuto_social'
        
        elif any(palavra in conteudo_lower for palavra in ['procedimento', 'processo', 'fluxo']):
            tipo = 'procedimento'
            if 'qualidade' in conteudo_lower:
                categoria = 'procedimento_qualidade'
            elif 'seguran√ßa' in conteudo_lower:
                categoria = 'procedimento_seguranca'
            else:
                categoria = 'procedimento_operacional'
        
        elif any(palavra in conteudo_lower for palavra in ['pol√≠tica', 'diretrizes', 'normas']):
            tipo = 'politica'
            categoria = 'politica_interna'
        
        else:
            tipo = 'documento'
            categoria = 'documento_geral'
        
        return tipo, categoria
    
    def extrair_tags(self, conteudo: str, tipo_documento: str) -> List[str]:
        """Extrai tags relevantes do documento"""
        tags = []
        conteudo_lower = conteudo.lower()
        
        # Tags gerais
        palavras_chave = [
            'qualidade', 'seguran√ßa', 'processo', 'gest√£o', 'controle',
            'auditoria', 'compliance', 'treinamento', 'comunica√ß√£o',
            'financeiro', 'recursos humanos', 'ti', 'tecnologia',
            'vendas', 'marketing', 'opera√ß√µes', 'log√≠stica'
        ]
        
        for palavra in palavras_chave:
            if palavra in conteudo_lower:
                tags.append(palavra)
        
        # Adiciona o tipo como tag
        tags.append(tipo_documento)
        
        # Remove duplicatas
        return list(set(tags))
    
    def processar_documento(self, caminho_arquivo: str, versao: str = "1.0", notas_versao: str = "") -> Dict[str, any]:
        """
        Processa um documento completo:
        1. L√™ o arquivo
        2. Cria chunks
        3. Gera embeddings
        4. Salva no Supabase
        """
        print(f"\nüîÑ Iniciando processamento de: {caminho_arquivo}")
        
        try:
            # L√™ o arquivo
            with open(caminho_arquivo, 'r', encoding='utf-8') as f:
                conteudo = f.read()
            
            if not conteudo.strip():
                raise ValueError("Arquivo vazio")
            
            # Extrai metadados
            nome_arquivo = os.path.basename(caminho_arquivo)
            hash_documento = self.calcular_hash_documento(conteudo)
            tipo_documento, categoria = self.detectar_tipo_documento(conteudo, nome_arquivo)
            tags = self.extrair_tags(conteudo, tipo_documento)
            
            print(f"üìã Tipo: {tipo_documento} | Categoria: {categoria}")
            print(f"üè∑Ô∏è  Tags: {', '.join(tags)}")
            
            # Verifica se documento j√° existe
            resultado_existente = self.supabase_client.table('base_conhecimento').select('id').eq(
                'hash_documento', hash_documento
            ).execute()
            
            if resultado_existente.data:
                print("‚ö†Ô∏è  Documento j√° processado com este conte√∫do")
                return {
                    'status': 'ja_existe',
                    'documento': nome_arquivo,
                    'hash': hash_documento
                }
            
            # Cria chunks
            chunks = self.criar_chunks(conteudo)
            
            # Processa cada chunk
            chunks_salvos = 0
            erros = []
            
            for i, chunk in enumerate(chunks):
                try:
                    print(f"üîÑ Processando chunk {i+1}/{len(chunks)}...", end='\r')
                    
                    # Gera embedding
                    embedding = self.gerar_embedding(chunk['conteudo'])
                    
                    # Prepara dados para inser√ß√£o
                    dados_chunk = {
                        'conteudo': chunk['conteudo'],
                        'embedding': embedding,
                        'chunk_index': chunk['chunk_index'],
                        'chunk_size': chunk['chunk_size'],
                        'documento_origem': nome_arquivo,
                        'tipo_documento': tipo_documento,
                        'categoria': categoria,
                        'tags': tags,
                        'versao_documento': versao,
                        'hash_documento': hash_documento,
                        'metadata': {
                            'processado_em': datetime.now().isoformat(),
                            'total_chunks': len(chunks),
                            'tamanho_original': len(conteudo)
                        }
                    }
                    
                    # Salva no Supabase
                    self.supabase_client.table('base_conhecimento').insert(dados_chunk).execute()
                    chunks_salvos += 1
                    
                except Exception as e:
                    erros.append(f"Chunk {i}: {str(e)}")
                    print(f"\n‚ùå Erro no chunk {i}: {e}")
            
            print(f"\n‚úÖ {chunks_salvos}/{len(chunks)} chunks salvos com sucesso")
            
            # Registra vers√£o do documento
            try:
                self.supabase_client.table('base_conhecimento_versoes').insert({
                    'documento_origem': nome_arquivo,
                    'versao': versao,
                    'hash_documento': hash_documento,
                    'notas_versao': notas_versao,
                    'usuario_upload': 'sistema'
                }).execute()
            except Exception as e:
                print(f"‚ö†Ô∏è  Aviso: Erro ao registrar vers√£o: {e}")
            
            return {
                'status': 'sucesso',
                'documento': nome_arquivo,
                'tipo': tipo_documento,
                'categoria': categoria,
                'tags': tags,
                'chunks_processados': chunks_salvos,
                'total_chunks': len(chunks),
                'hash': hash_documento,
                'erros': erros
            }
            
        except Exception as e:
            print(f"\n‚ùå Erro ao processar documento: {e}")
            return {
                'status': 'erro',
                'documento': caminho_arquivo,
                'erro': str(e)
            }
    
    def processar_pasta(self, caminho_pasta: str) -> List[Dict[str, any]]:
        """Processa todos os arquivos TXT em uma pasta"""
        resultados = []
        pasta = Path(caminho_pasta)
        
        if not pasta.exists():
            print(f"‚ùå Pasta n√£o encontrada: {caminho_pasta}")
            return resultados
        
        arquivos_txt = list(pasta.glob("*.txt"))
        
        if not arquivos_txt:
            print(f"‚ö†Ô∏è  Nenhum arquivo TXT encontrado em: {caminho_pasta}")
            return resultados
        
        print(f"üìÅ Encontrados {len(arquivos_txt)} arquivos TXT para processar")
        
        for arquivo in arquivos_txt:
            resultado = self.processar_documento(str(arquivo))
            resultados.append(resultado)
        
        return resultados
    
    def buscar_conhecimento(self, consulta: str, limite: int = 5, filtros: Dict = None) -> List[Dict]:
        """
        Busca conhecimento similar usando embeddings
        """
        try:
            # Gera embedding da consulta
            embedding_consulta = self.gerar_embedding(consulta)
            
            # Prepara par√¢metros da fun√ß√£o SQL
            params = {
                'query_embedding': embedding_consulta,
                'limite': limite
            }
            
            if filtros:
                if 'tipo_documento' in filtros:
                    params['tipo_doc'] = filtros['tipo_documento']
                if 'categoria' in filtros:
                    params['categoria_filtro'] = filtros['categoria']
                if 'tags' in filtros:
                    params['tags_filtro'] = filtros['tags']
            
            # Chama fun√ß√£o SQL
            resultado = self.supabase_client.rpc('buscar_conhecimento_similar', params).execute()
            
            return resultado.data
            
        except Exception as e:
            print(f"‚ùå Erro na busca: {e}")
            return []
    
    def listar_documentos(self) -> List[Dict]:
        """Lista todos os documentos processados"""
        try:
            resultado = self.supabase_client.table('base_conhecimento').select(
                'documento_origem, tipo_documento, categoria, versao_documento'
            ).eq('ativo', True).execute()
            
            # Agrupa por documento √∫nico
            documentos = {}
            for item in resultado.data:
                doc = item['documento_origem']
                if doc not in documentos:
                    documentos[doc] = {
                        'documento': doc,
                        'tipo': item['tipo_documento'],
                        'categoria': item['categoria'],
                        'versao': item['versao_documento']
                    }
            
            return list(documentos.values())
            
        except Exception as e:
            print(f"‚ùå Erro ao listar documentos: {e}")
            return []


# Teste r√°pido
if __name__ == "__main__":
    processador = ProcessadorBaseConhecimento()
    
    # Teste de chunking
    texto_teste = """Este √© um manual de procedimentos da empresa.
    O manual cont√©m instru√ß√µes detalhadas sobre os processos de qualidade.
    Cada procedimento deve ser seguido rigorosamente para garantir a conformidade.
    Este documento √© parte do sistema de gest√£o da qualidade."""
    
    chunks = processador.criar_chunks(texto_teste)
    print(f"\nTeste de chunking: {len(chunks)} chunks criados")
    
    # Teste de detec√ß√£o
    tipo, categoria = processador.detectar_tipo_documento(texto_teste, "manual_qualidade.txt")
    print(f"Tipo detectado: {tipo} | Categoria: {categoria}")
    
    # Teste de tags
    tags = processador.extrair_tags(texto_teste, tipo)
    print(f"Tags extra√≠das: {tags}")